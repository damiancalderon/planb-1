{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline para Despliegue de Modelos de Machine Learning\n",
        "\n",
        "\n",
        "## 1. Recolección de Datos\n",
        "Obtén los datos necesarios para tu modelo, ya sea desde bases de datos, archivos CSV o APIs.\n",
        "\n",
        "## 2. Exploración y Análisis de Datos\n",
        "Realiza un análisis descriptivo y visual para entender la estructura de los datos, detectar valores faltantes, outliers y relaciones entre variables.\n",
        "\n",
        "## 3. Limpieza y Preprocesamiento de Datos\n",
        "- **Manejo de Valores Faltantes**: Imputación o eliminación de valores faltantes.\n",
        "- **Codificación de Variables Categóricas**: Utiliza técnicas como one-hot encoding si tienes variables categóricas.\n",
        "- **Normalización o Estandarización**: Dependiendo del modelo, este paso puede ser opcional.\n",
        "\n",
        "## 4. División de Datos\n",
        "Divide los datos en conjuntos de entrenamiento y prueba.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "## 5. Selección y Creación del Modelo\n",
        "\n",
        "Árboles de Decisión\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "Prophet\n",
        "\n",
        "```python\n",
        "from fbprophet import Prophet\n",
        "\n",
        "train = train.rename(columns={'date': 'ds', 'value': 'y'})\n",
        "model = Prophet()\n",
        "model.fit(train)\n",
        "```\n",
        "XGBoost\n",
        "```python\n",
        "import xgboost as xgb\n",
        "\n",
        "model = xgb.XGBClassifier()  # o xgb.XGBRegressor() para regresión\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "## 6. Optimización de Hiperparámetros\n",
        "\n",
        "Utiliza Grid Search o Random Search para encontrar los mejores hiperparámetros.\n",
        "\n",
        "Ejemplo con XGBoost\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "```\n",
        "## 7. Evaluación del Modelo\n",
        "\n",
        "Evalúa el modelo utilizando el conjunto de prueba y métricas de evaluación apropiadas.\n",
        "\n",
        "Ejemplo con XGBoost\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "## 8. Despliegue del Modelo\n",
        "\n",
        "Despliega el modelo en producción utilizando herramientas como Flask o FastAPI para crear una API que sirva las predicciones del modelo.\n",
        "Ejemplo de Despliegue\n",
        "```python\n",
        "import pickle\n",
        "\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "\n",
        "# Ejemplo usando Flask\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json(force=True)\n",
        "    prediction = best_model.predict([data['input']])\n",
        "    return jsonify({'prediction': prediction[0]})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000, debug=True)\n",
        "```\n",
        "\n",
        "## 9. Monitoreo y Mantenimiento\n",
        "\n",
        "Monitorea el rendimiento del modelo y realiza mantenimiento continuo.\n",
        "Monitoreo\n",
        "\n",
        "**Seguimiento de Rendimiento**: Monitorear métricas de rendimiento como precisión, recall, F1-score, MAE, RMSE, etc.\n",
        "**Monitoreo de Datos**: Asegurar que los datos de entrada siguen siendo consistentes con los datos usados para entrenar el modelo.\n",
        "**Alertas y Notificaciones**: Configurar alertas para notificar cuando el rendimiento del modelo cae por debajo de un umbral predeterminado.\n",
        "\n",
        "Mantenimiento\n",
        "\n",
        "**Retraining**: Reentrenar el modelo periódicamente con nuevos datos para asegurar que se mantenga actualizado.\n",
        "**Versionado del Modelo**: Mantener versiones del modelo para poder revertir a versiones anteriores si surgen problemas con la nueva versión.\n",
        "**Validación Continua**: Validar regularmente el modelo contra datos de prueba para garantizar que sigue cumpliendo con los requisitos de rendimiento.\n",
        "**Documentación y Revisión**: Documentar todas las actualizaciones, cambios y reentrenamientos del modelo. Revisar periódicamente la documentación y los procesos para mejorar la eficiencia y la efectividad del pipeline de mantenimiento.\n"
      ],
      "metadata": {
        "id": "gzaloChF4Km6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJQ5lIDs3-pB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4aebc1-054e-4455-ed1f-6e1ab62b09e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "#example\n",
        "import joblib\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target # Target (species)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier with 100 estimators (trees)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "# Calculate accuracy\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Load the model from the file\n",
        "#loaded_model = joblib.load('random_forest_model.joblib')\n",
        "\n",
        "# Now you can use the loaded model to make predictions\n",
        "#predictions = loaded_model.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'random_fores.joblib'\n",
        "joblib.dump(clf, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIecpnQgQqMq",
        "outputId": "29aaf839-8c90-464d-d486-215c6d2089f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['random_fores.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = joblib.load('random_fores.joblib')\n",
        "predictions = loaded_model.predict(X)"
      ],
      "metadata": {
        "id": "R73nqzB3QsoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)"
      ],
      "metadata": {
        "id": "NCLwOsaTRDZb",
        "outputId": "4463d328-dbd4-4463-bede-c0046ec8e34d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ]
        }
      ]
    }
  ]
}